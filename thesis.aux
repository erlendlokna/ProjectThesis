\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{anyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand \oddpage@label [2]{}
\babel@aux{british}{}
\@writefile{toc}{\contentsline {chapter}{Abstract}{v}{Doc-Start}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@cite{0}{logcalc}
\abx@aux@segm{0}{0}{logcalc}
\abx@aux@cite{0}{perceptron}
\abx@aux@segm{0}{0}{perceptron}
\abx@aux@cite{0}{backprop}
\abx@aux@segm{0}{0}{backprop}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical background}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Neural Networks}{2}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Illustration of a 2 layered MLP with $N$ inputs, $K$ hidden units, weights $W_1$ and $W_2$ and a single output $y$.\relax }}{2}{figure.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Learning in Neural Networks}{3}{subsection.2.1.1}\protected@file@percent }
\abx@aux@cite{0}{SGD}
\abx@aux@segm{0}{0}{SGD}
\abx@aux@cite{0}{backprop2}
\abx@aux@segm{0}{0}{backprop2}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Supervised and unsupervised learning}{4}{section.2.2}\protected@file@percent }
\abx@aux@cite{0}{LeChun}
\abx@aux@segm{0}{0}{LeChun}
\abx@aux@cite{0}{Fukushima}
\abx@aux@segm{0}{0}{Fukushima}
\abx@aux@cite{0}{HubelWiesel}
\abx@aux@segm{0}{0}{HubelWiesel}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks, CNN}{5}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Convolutional layer}{5}{subsection.2.3.1}\protected@file@percent }
\abx@aux@cite{0}{RiebesellTikZ2022}
\abx@aux@segm{0}{0}{RiebesellTikZ2022}
\abx@aux@cite{0}{RiebesellTikZ2022}
\abx@aux@segm{0}{0}{RiebesellTikZ2022}
\newlabel{eq:activation}{{2.9}{6}{Convolutional layer}{equation.2.9}{}}
\newlabel{eq:activation@cref}{{[equation][9][2]2.9}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of convolutional operation. $\mathbf  {I}$ is the input, $\mathbf  {K}$ is the kernel and $\mathbf  {I} * \mathbf  {K}$ is the activation map. Illustration taken from the Random TikZ collection\blx@tocontentsinit {0}\cite {RiebesellTikZ2022}\relax }}{6}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:conv}{{2.2}{6}{Example of convolutional operation. $\mathbf {I}$ is the input, $\mathbf {K}$ is the kernel and $\mathbf {I} * \mathbf {K}$ is the activation map. Illustration taken from the Random TikZ collection\cite {RiebesellTikZ2022}\relax }{figure.caption.3}{}}
\newlabel{fig:conv@cref}{{[figure][2][2]2.2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Pooling and Downsampling}{6}{subsection.2.3.2}\protected@file@percent }
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Illustration of a max pooling operation. The input feature map is reduced in size by applying a max pooling filter with size 2x2 (red boxes), which selects the maximum value in each filter region to produce the max pooled feature map.\relax }}{7}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Architecture of CNNs}{7}{subsection.2.3.3}\protected@file@percent }
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Residual block}{8}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Illustration of the Resnet block from He et al's original paper\blx@tocontentsinit {0}\cite {ResLearn} \relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:resblock}{{2.4}{8}{Illustration of the Resnet block from He et al's original paper\cite {ResLearn} \relax }{figure.caption.5}{}}
\newlabel{fig:resblock@cref}{{[figure][4][2]2.4}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Self Supervised Learning, SSL}{8}{section.2.5}\protected@file@percent }
\abx@aux@cite{0}{MoCo}
\abx@aux@segm{0}{0}{MoCo}
\abx@aux@cite{0}{SimCLR}
\abx@aux@segm{0}{0}{SimCLR}
\abx@aux@cite{0}{BYOL}
\abx@aux@segm{0}{0}{BYOL}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{Siamese}
\abx@aux@segm{0}{0}{Siamese}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Contrastive and Non Contrastive learning}{9}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}The Role of Siamese Networks in SSL}{9}{subsection.2.5.2}\protected@file@percent }
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Example of a non contrastive siamese network using augmented views. Here the images are processed to calculate a similarity score, the backpropagation arrow indicates that the similarity score is then used to update the weights of the sub network. Pictures taken from \url  {www.pexels.com}\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:Siamese}{{2.5}{10}{Example of a non contrastive siamese network using augmented views. Here the images are processed to calculate a similarity score, the backpropagation arrow indicates that the similarity score is then used to update the weights of the sub network. Pictures taken from \url {www.pexels.com}\relax }{figure.caption.6}{}}
\newlabel{fig:Siamese@cref}{{[figure][5][2]2.5}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Barlow Twins}{10}{subsection.2.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Objective function of Barlow Twins}{10}{section*.7}\protected@file@percent }
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{VAE}
\abx@aux@segm{0}{0}{VAE}
\abx@aux@cite{0}{Rumelhart}
\abx@aux@segm{0}{0}{Rumelhart}
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\@writefile{toc}{\contentsline {subsubsection}{Siamese architecture}{11}{section*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Illustration of the Barlow Twins procedure, inspired by the original paper\blx@tocontentsinit {0}\cite {Barlow}.\relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:Barlow}{{2.6}{11}{Illustration of the Barlow Twins procedure, inspired by the original paper\cite {Barlow}.\relax }{figure.caption.9}{}}
\newlabel{fig:Barlow@cref}{{[figure][6][2]2.6}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}The Vector Quantized Variational Autoencoder, VQ-VAE}{12}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Evolution from Variational Auto-Encoder, VAE}{12}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational inference and optimization}{12}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges in Maximum Likelihood Estimation}{12}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational Approximation}{13}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Evidence Lower Bound}{13}{section*.13}\protected@file@percent }
\newlabel{eq:ELBO}{{2.15}{13}{Evidence Lower Bound}{equation.2.15}{}}
\newlabel{eq:ELBO@cref}{{[equation][15][2]2.15}{[1][13][]13}}
\abx@aux@cite{0}{VAE}
\abx@aux@segm{0}{0}{VAE}
\abx@aux@cite{0}{1312.6114}
\abx@aux@segm{0}{0}{1312.6114}
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Transition to discrete latent variables}{14}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Vector quantization}{14}{section*.14}\protected@file@percent }
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\abx@aux@cite{0}{posteriorcollapse}
\abx@aux@segm{0}{0}{posteriorcollapse}
\@writefile{toc}{\contentsline {subsubsection}{Effect on KL divergence}{15}{section*.15}\protected@file@percent }
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\abx@aux@cite{0}{VQVAE-2}
\abx@aux@segm{0}{0}{VQVAE-2}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\abx@aux@cite{0}{cvae}
\abx@aux@segm{0}{0}{cvae}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\abx@aux@cite{0}{UCRArchive2018}
\abx@aux@segm{0}{0}{UCRArchive2018}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related work / Literature review}{16}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{fill}{16}{section*.16}\protected@file@percent }
\pgfsyspdfmark {pgfid3}{10734166}{21364465}
\pgfsyspdfmark {pgfid6}{36067557}{21366977}
\pgfsyspdfmark {pgfid7}{38009087}{21133572}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\abx@aux@cite{0}{ResLearn}
\abx@aux@segm{0}{0}{ResLearn}
\abx@aux@cite{0}{batchnorm}
\abx@aux@segm{0}{0}{batchnorm}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methology}{18}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Encoder}{18}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustration of the Encoder architecture.\relax }}{19}{figure.caption.17}\protected@file@percent }
\newlabel{fig:Encoder}{{4.1}{19}{Illustration of the Encoder architecture.\relax }{figure.caption.17}{}}
\newlabel{fig:Encoder@cref}{{[figure][1][4]4.1}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Decoder}{19}{section.4.2}\protected@file@percent }
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\abx@aux@cite{0}{lee2023masked}
\abx@aux@segm{0}{0}{lee2023masked}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}VQVAE}{20}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Reconstruction}{20}{subsection.4.3.1}\protected@file@percent }
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  Illustration of the VQVAE. Illustration and implementation inspired by the TimeVQVAE paper\blx@tocontentsinit {0}\cite {lee2023masked} \relax }}{21}{figure.caption.18}\protected@file@percent }
\newlabel{fig:VQVAE}{{4.2}{21}{Illustration of the VQVAE. Illustration and implementation inspired by the TimeVQVAE paper\cite {lee2023masked} \relax }{figure.caption.18}{}}
\newlabel{fig:VQVAE@cref}{{[figure][2][4]4.2}{[1][20][]21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Learning}{21}{subsection.4.3.2}\protected@file@percent }
\newlabel{eq:VQVAEloss}{{4.1}{21}{Learning}{equation.4.1}{}}
\newlabel{eq:VQVAEloss@cref}{{[equation][1][4]4.1}{[1][21][]21}}
\@writefile{toc}{\contentsline {subsubsection}{Reconstruction}{21}{section*.19}\protected@file@percent }
\newlabel{eq:recon}{{4.3}{21}{Reconstruction}{equation.4.3}{}}
\newlabel{eq:recon@cref}{{[equation][3][4]4.3}{[1][21][]21}}
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\abx@aux@cite{0}{neuvqvae}
\abx@aux@segm{0}{0}{neuvqvae}
\@writefile{toc}{\contentsline {subsubsection}{Codebook-learning loss}{22}{section*.20}\protected@file@percent }
\newlabel{eq:codebook}{{4.4}{22}{Codebook-learning loss}{equation.4.4}{}}
\newlabel{eq:codebook@cref}{{[equation][4][4]4.4}{[1][22][]22}}
\@writefile{toc}{\contentsline {subsubsection}{Codebook Update Mechanism}{22}{section*.21}\protected@file@percent }
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualization of the embedding space inspired by the original VQVAE paper\blx@tocontentsinit {0}\cite {neuvqvae}. The continous $z_{ij}$, in blue, gets mapped to the nearest token $z_k$. The gradient $\nabla _z \mathcal  {L}$ in the backpropagation is copied from the decoder to the encoder. \relax }}{23}{figure.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Modifying the VQVAE for Enhanced Self Supervised Learning}{23}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Modifying the VQVAE encoder}{23}{subsection.4.4.1}\protected@file@percent }
\abx@aux@cite{0}{VICReg}
\abx@aux@segm{0}{0}{VICReg}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\abx@aux@cite{0}{VICReg}
\abx@aux@segm{0}{0}{VICReg}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Illustration of the Barlow Twins modification to VQVAE including the loss function calculation.\relax }}{24}{figure.caption.23}\protected@file@percent }
\newlabel{fig:BTVQVAE}{{4.4}{24}{Illustration of the Barlow Twins modification to VQVAE including the loss function calculation.\relax }{figure.caption.23}{}}
\newlabel{fig:BTVQVAE@cref}{{[figure][4][4]4.4}{[1][24][]24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Projector}{24}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Learning}{24}{section*.24}\protected@file@percent }
\abx@aux@cite{0}{Barlow}
\abx@aux@segm{0}{0}{Barlow}
\abx@aux@cite{0}{augs}
\abx@aux@segm{0}{0}{augs}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\newlabel{eq:BTVQVAEloss}{{4.8}{25}{Learning}{equation.4.8}{}}
\newlabel{eq:BTVQVAEloss@cref}{{[equation][8][4]4.8}{[1][25][]25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Augmentation Techniques}{25}{subsection.4.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Visualization of the flip, slope and STFT augmentation aplied on a timeseries. \relax }}{27}{figure.caption.27}\protected@file@percent }
\abx@aux@cite{0}{UCRArchive2018}
\abx@aux@segm{0}{0}{UCRArchive2018}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Setup}{28}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}UCR Archive}{28}{section.5.1}\protected@file@percent }
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\abx@aux@cite{0}{SSLs}
\abx@aux@segm{0}{0}{SSLs}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Chosen UCR Archive subset. Data collected by Lee et al\blx@tocontentsinit {0}\cite {SSLs}\relax }}{29}{table.caption.28}\protected@file@percent }
\newlabel{tab:sample_table}{{5.1}{29}{Chosen UCR Archive subset. Data collected by Lee et al\cite {SSLs}\relax }{table.caption.28}{}}
\newlabel{tab:sample_table@cref}{{[table][1][5]5.1}{[1][29][]29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Reconstruction evaluation}{29}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Downstream evaluation}{29}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Extracting Discrete latent variables}{30}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Illustration showing the processing of training and validation datasets to latent representations.\relax }}{30}{figure.caption.29}\protected@file@percent }
\newlabel{fig:latents}{{5.1}{30}{Illustration showing the processing of training and validation datasets to latent representations.\relax }{figure.caption.29}{}}
\newlabel{fig:latents@cref}{{[figure][1][5]5.1}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Downstream tests}{30}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results and discussion}{32}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{33}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\abx@aux@read@bbl@mdfivesum{4804369400F6F9655D703523BACBDF81}
\abx@aux@defaultrefcontext{0}{cvae}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VICReg}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{SimCLR}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{UCRArchive2018}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{perceptron}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Fukushima}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{BYOL}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ResLearn}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MoCo}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{HubelWiesel}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{batchnorm}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{1312.6114}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VAE}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{SSLs}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lee2023masked}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{LeChun}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Siamese}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{logcalc}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{neuvqvae}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Rumelhart}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{backprop2}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{RiebesellTikZ2022}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{SGD}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VQVAE-2}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{posteriorcollapse}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{augs}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{backprop}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Barlow}{anyt/global//global/global}
\gdef \@abspage@last{42}
